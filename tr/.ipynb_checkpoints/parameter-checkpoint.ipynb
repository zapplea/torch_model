{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# type of parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paramter is a subclass of Variable. It can be registerd automatically in Module.\n",
      "=========\n",
      "Linear(in_features=3, out_features=4, bias=True)\n",
      "=========\n",
      "W\n",
      "Parameter containing:\n",
      " 0.1846  2.2711  2.1454  1.5377\n",
      "-0.0805 -0.4595  1.0433  0.2679\n",
      " 1.9543 -0.3213 -0.6220 -0.5283\n",
      "[torch.FloatTensor of size 3x4]\n",
      "\n",
      "linears.0.weight\n",
      "Parameter containing:\n",
      " 0.0668 -0.4425 -0.3781\n",
      " 0.2055 -0.2379  0.2165\n",
      "-0.2172 -0.3258 -0.5064\n",
      " 0.3902 -0.1524  0.2927\n",
      "[torch.FloatTensor of size 4x3]\n",
      "\n",
      "linears.0.bias\n",
      "Parameter containing:\n",
      "-0.2082\n",
      "-0.5160\n",
      " 0.4034\n",
      " 0.2254\n",
      "[torch.FloatTensor of size 4]\n",
      "\n",
      "linears.1.weight\n",
      "Parameter containing:\n",
      "-0.3648  0.3214  0.1071 -0.2425\n",
      "-0.0786  0.0935 -0.1041 -0.2543\n",
      " 0.1297  0.3702  0.0058  0.3749\n",
      "-0.1895  0.0549 -0.1241 -0.1142\n",
      "-0.3741 -0.0767  0.3798 -0.3648\n",
      " 0.0458 -0.4047  0.4034  0.2356\n",
      "[torch.FloatTensor of size 6x4]\n",
      "\n",
      "linears.1.bias\n",
      "Parameter containing:\n",
      "-0.2079\n",
      " 0.4595\n",
      " 0.4174\n",
      " 0.2338\n",
      " 0.2250\n",
      " 0.2661\n",
      "[torch.FloatTensor of size 6]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch as tr\n",
    "import numpy as np\n",
    "class Net(tr.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        self.linears = tr.nn.ModuleList([tr.nn.Linear(3,4),tr.nn.Linear(4,6)])\n",
    "        self.W = tr.nn.Parameter(tr.FloatTensor(np.random.randn(3,4)),requires_grad=True)\n",
    "        print('=========')\n",
    "        print(self.linears[0])\n",
    "        print('=========')\n",
    "\n",
    "print('Paramter is a subclass of Variable. It can be registerd automatically in Module.')\n",
    "module = Net()\n",
    "for name,para in list(module.named_parameters()):\n",
    "    print(name)\n",
    "    print(para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 3  3  3  3\n",
      " 3  3  3  3\n",
      " 3  3  3  3\n",
      "[torch.FloatTensor of size 3x4]\n",
      "\n",
      "W\n",
      "Parameter containing:\n",
      " 1  1  1  1\n",
      " 1  1  1  1\n",
      " 1  1  1  1\n",
      "[torch.FloatTensor of size 3x4]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch as tr\n",
    "import numpy as np\n",
    "class Net(tr.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        self.W = tr.nn.Parameter(tr.Tensor(np.ones((3,4))))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        z = tr.matmul(X,self.W)\n",
    "        return z\n",
    "\n",
    "module = Net()\n",
    "X = tr.autograd.Variable(tr.Tensor(np.ones((3,3),dtype='float32')),requires_grad=False)\n",
    "outputs = module(X)\n",
    "print(outputs)\n",
    "for name,para in list(module.named_parameters()):\n",
    "    print(name)\n",
    "    print(para)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter will not be registered if it is created outside __init__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 3  3  3  3\n",
      " 3  3  3  3\n",
      " 3  3  3  3\n",
      "[torch.FloatTensor of size 3x4]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch as tr\n",
    "import numpy as np\n",
    "class Net(tr.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "    \n",
    "    def forward(self, X):\n",
    "        W = tr.nn.Parameter(tr.Tensor(np.ones((3,4))))\n",
    "        z = tr.matmul(X,W)\n",
    "        return z\n",
    "\n",
    "module = Net()\n",
    "X = tr.autograd.Variable(tr.Tensor(np.ones((3,3),dtype='float32')),requires_grad=False)\n",
    "outputs = module(X)\n",
    "print(outputs)\n",
    "for name,para in list(module.named_parameters()):\n",
    "    print(name)\n",
    "    print(para)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Is forward function necessary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NLLLoss' object has no attribute 'backward'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-ddf9a8df162d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    396\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 398\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NLLLoss' object has no attribute 'backward'"
     ]
    }
   ],
   "source": [
    "import torch as tr\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "class Net(tr.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        self.linears = tr.nn.ModuleList([tr.nn.Linear(10,100),tr.nn.Linear(10,50)])\n",
    "    \n",
    "    def forward_softmax(self,X):\n",
    "        linear_layer = self.linears[0](X)\n",
    "        outputs = F.log_softmax(linear_layer,dim=1)\n",
    "        return outputs\n",
    "    \n",
    "    def cross_entropy_loss(self,score,target):\n",
    "        loss = tr.nn.NLLLoss(size_average=True,reduce=True)\n",
    "        return loss(score,target)\n",
    "        \n",
    "    \n",
    "\n",
    "# \n",
    "def optimizer(module):\n",
    "    optim = tr.optim.SGD(module.parameters(),lr=0.03,weight_decay=0.0003)\n",
    "    return optim\n",
    "    \n",
    "module = Net()\n",
    "optim = optimizer(module)\n",
    "X = tr.autograd.Variable(tr.Tensor(np.random.randn(30,10)),requires_grad=False)\n",
    "y_ = tr.autograd.Variable(tr.LongTensor(np.random.randint(low=0,high=3,size=(30))))\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    optim.zero_grad()\n",
    "    score = module.forward_softmax(X)\n",
    "    loss = module.cross_entropy_loss(score,y_)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    for i,j in module.named_parameters():\n",
    "        print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
